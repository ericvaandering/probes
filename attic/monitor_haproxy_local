#!/usr/bin/env python
# Copyright European Organization for Nuclear Research (CERN) 2013
#
# Licensed under the Apache License, Version 2.0 (the "License");
# You may not use this file except in compliance with the License.
# You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
#
# Authors:
# - Ralph Vigne <ralph.vigne@cern.ch>, 2014
# - Thomas Beermann, <thomas.beermann@cern.ch>, 2019


import argparse
import socket
import traceback
import logging
import logging.handlers
import sys

from prometheus_client import CollectorRegistry, Gauge, push_to_gateway
from pystatsd import Client
from sys import stdout

# Define logger
logging.basicConfig(stream=stdout,
                    level=logging.INFO,
                    format='%(asctime)s\t%(process)d\t%(levelname)s\t%(message)s')

logger = logging.getLogger('rucio-haproxy-monitoring')

# Adding syslog handler
handler = logging.handlers.SysLogHandler('/dev/log')
handler.setFormatter(logging.Formatter('%(name)s[%(process)d] %(message)s'))
logger.addHandler(handler)


def monitor_haproxy(socket_name):
    data = {}
    INCLUDE_INFO = ['Process_num', 'Idle_pct']
    INCLUDE_STAT = ['scur', 'qcur', 'chkfail', 'status', 'weight', 'rate', 'hrsp_1xx', 'hrsp_2xx', 'hrsp_3xx', 'hrsp_4xx', 'hrsp_5xx', 'req_rate', 'qtime', 'ctime', 'rtime', 'ttime']

    # Request data from socket
    logger.debug('Connecting to socket: %s' % socket_name)
    try:
        s = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        s.connect(socket_name)
        logger.debug('Requesting info')
        s.send('show info\n')
        raw_info = s.recv(4096)
        s.close()  # Note: socket is not reusable
        logger.debug('Requesting stat')
        s = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        s.connect(socket_name)
        s.send('show stat\n')
        raw_stat = s.recv(8192)
        s.close()
    except Exception as e:
        logger.error('Failed requesting data from socket %s with execption %s' % (socket_name, e))
        logger.error(traceback.format_exc(e))
        return None
    logger.debug('Successfully requested data from socket.')

    # Transforming info response into dictonary
    logger.debug('Parsing info response')
    for entry in raw_info.split('\n'):
        tmp = entry.split(': ')
        try:
            if tmp[0] in INCLUDE_INFO:
                data[tmp[0]] = float(tmp[1])
        except Exception as e:
            logger.error('Entry: %s failed with exception: %s' % (tmp, e))
            logger.error(traceback.format_exc(e))
    logger.debug('Done parsing info response.')

    # Transforming stat response into dictonary
    logger.debug('Parsing stat response')
    raw_stat = raw_stat.split('\n')
    headers = raw_stat.pop(0).split(',')[2:-1]  # Get the column headers and remove pxname and svname
    for stat in raw_stat:
        stat = stat.split(',')
        if len(stat) == 1:
            logger.debug('Ignored line: %s' % stat[0])
            continue  # Line is something else than stats
        prefix = '%s.%s' % (stat.pop(0), stat.pop(0))  # Build metric prefix using pxname and svname
        for column in range(len(headers)):
            try:
                if headers[column] in INCLUDE_STAT:
                    if (headers[column] == 'status') and (stat[column] in ['UP', 'DOWN', 'MAINT']) and (data['Process_num'] == 1.0):
                        for s in ['UP', 'DOWN', 'MAINT']:
                            data[prefix + '.' + headers[column] + '.' + s] = 0  # set all status to zero to support gauge values
                        data[prefix + '.' + headers[column] + '.' + stat[column]] = 1
                    else:
                        data[prefix + '.' + headers[column]] = float(stat[column])
            except Exception as e:
                logger.debug('Igonring data: %s -> %s' % (headers[column], stat[column]))
    logger.debug('Done parsing stat response.')
    return data


def backend_graphite(url, stats, prefix):
    process_num = stats['Process_num']
    server_name = socket.getfqdn().split('.')[0]
    prefix = '%s.%s.%s' % (prefix, server_name, int(process_num))
    logger.debug('Reporting to prefix: %s' % prefix)
    server, port = url.split(':')
    try:
        pystatsd_client = Client(host=server, port=port, prefix=prefix)
    except Exception, e:
        logger.error('Unable to connect to Graphite backend %s: %s' % (url, e))
        raise

    for s in stats:
        if s == 'Process_num':
            continue
        try:
            pystatsd_client.gauge(s, float(stats[s]))
            logger.debug('%s.%s => %s' % (prefix, s, float(stats[s])))
        except Exception as e:
            logger.error('Failed reporting %s (%s): %s' % (s, stats[s], e))


def backend_prometheus(urls, stats, prefix):
    registry = CollectorRegistry()
    process_num = stats['Process_num']
    del(stats['Process_num'])
    server_name = socket.getfqdn().split('.')[0]

    labelnames = ('name', 'endpoint', 'process_num', 'server_name')
    gauges = {}
    for s in stats:
        stat = s
        items = s.split('.')

        name = 'none'
        endpoint = 'none'
        if len(items) > 2:
            s = items[2]
            name = items[0]
            endpoint = items[1]
        if len(items) == 4:
            s = "_".join(items[2:4])

        if s not in gauges:
            gauges[s] = Gauge("_".join((prefix.replace(".", "_"), s)), '', labelnames=labelnames, registry=registry)

        gauges[s].labels(**{'name': name, 'endpoint': endpoint, 'process_num': process_num, 'server_name': server_name}).set(float(stats[stat]))

    for url in urls:
        try:
            logging.debug("try to push to %s" % url)
            push_to_gateway(url.strip(), job='monitor_haproxy_' + server_name, registry=registry)
        except:
            continue


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--backend', metavar='B', type=str, nargs=1, help=' Graphite server URL[:port][::scope] to which the script will report to. E.g. --backend rucio-graphite-int.cern.ch:8025/listen/now::rucio.loadbalancer')
    parser.add_argument('--prometheus_servers', metavar='P', type=str, nargs='+', help='a list of prometheus pushgateway servers')
    parser.add_argument('--sockets', metavar='S', type=str, nargs='+', help='a list of socket files e.g. /var/run/haproxy_admin_process_no_1.sock')
    parser.add_argument('--verbose', help='makes it chatty', action="store_true")

    args = parser.parse_args()

    if args.verbose:
        logger.setLevel(level=logging.DEBUG)
    args = vars(args)

    try:
        url, prefix = args['backend'][0].split('::')
        logger.debug('Reporting to backend => URL: %s\tPrefix: %s' % (url, prefix))
    except ValueError:
        logger.critical('Can not unpack backend information: %s' % args['backend'][0])
        sys.exit(1)

    prometheus_servers = None
    if args['prometheus_servers']:
        try:
            prometheus_servers = args['prometheus_servers'][0].split(',')
        except ValueError:
            logger.critical('Can not unpack prometheus server information: %s' % args['prometheus_servers'][0])
            sys.exit(1)

    logger.info('Cheking sockets: %s' % args['sockets'])
    for socket_name in args['sockets']:
        try:
            data = monitor_haproxy(socket_name)
            backend_graphite(url, data, prefix)
            if prometheus_servers:
                backend_prometheus(prometheus_servers, data, prefix)
        except Exception as e:
            logger.error(e)
            logger.error(traceback.format_exc(e))
            sys.exit(1)
    sys.exit(0)
